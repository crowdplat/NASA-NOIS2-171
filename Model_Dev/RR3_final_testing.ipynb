{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.networks.nets import DenseNet121\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = 1\n",
    "image_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Scratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Scratch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Downsampling\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # Downsampling\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # Downsampling\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)  # Downsampling\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((2, 2))  # Ensure same feature size as DenseNet\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2, 128)  # Adjusted for the new feature size\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(torch.relu(self.conv4(x)))\n",
    "\n",
    "        x = self.global_avg_pool(x)  # Adaptive pooling ensures fixed feature size\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten correctly\n",
    "        # print(f\"Flattened feature size: {x.shape}\")  # Debug print\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransferLearningImageClassifier(nn.Module):\n",
    "    \"\"\" Image Classification Model using MONAI DenseNet121 \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(TransferLearningImageClassifier, self).__init__()\n",
    "        self.model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=num_classes)\n",
    "        \n",
    "        # Modify the pooling layer to ensure fixed feature extraction\n",
    "        self.model.features[-1] = nn.AdaptiveAvgPool2d((2, 2))  # Replace last pooling layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# class TransferLearningModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(TransferLearningModel, self).__init__()\n",
    "#         # Load pre-trained ResNet18 with weights\n",
    "#         self.base_model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "#         # Modify the first convolutional layer to accept grayscale input\n",
    "#         self.base_model.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1) \n",
    "        \n",
    "#         # Replace the fully connected layer for binary classification\n",
    "#         num_features = self.base_model.fc.in_features\n",
    "#         self.base_model.fc = nn.Sequential(\n",
    "#             nn.Linear(num_features, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(128, 1),\n",
    "#             nn.Sigmoid()  # Binary classification\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.base_model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class FullImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image as a NumPy array\n",
    "        image = np.load(self.image_paths[idx])  # Shape: [224, 224]\n",
    "        \n",
    "        # Scale pixel values to [0, 255] if they are normalized\n",
    "        if image.max() <= 1.0:  # Check if image is normalized\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        \n",
    "        # Convert NumPy array to PIL Image\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # # Convert to PyTorch tensor\n",
    "        # image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Convert label to PyTorch tensor\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LOOCV Function\n",
    "def loocv_full_image_with_augmentation(image_paths, labels, num_epochs=500, learning_rate=0.0001, save_dir=\"../Models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the save directory exists\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    true_labels = []\n",
    "    image_pred_prob = []\n",
    "    image_pred_label = []\n",
    "\n",
    "    # Define augmentations for the training dataset\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),             # Resize all images\n",
    "        transforms.RandomRotation(10),           # Rotate randomly within Â±10 degrees\n",
    "        # transforms.RandomResizedCrop(size=image_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "        transforms.ToTensor(),                   # Convert PIL Image to PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Define transform for test dataset (no augmentation, only normalization)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),             # Resize all images\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    for test_image_id in range(len(image_paths)):\n",
    "        print(f\"Processing LOOCV for test image {test_image_id + 1}/{len(image_paths)}\")\n",
    "\n",
    "        # Split dataset into training and test sets\n",
    "        train_images = [image_paths[i] for i in range(len(image_paths)) if i != test_image_id]\n",
    "        train_labels = [labels[i] for i in range(len(labels)) if i != test_image_id]\n",
    "        test_image = image_paths[test_image_id]\n",
    "        test_label = labels[test_image_id]\n",
    "\n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = FullImageDataset(train_images, train_labels, transform=train_transform)\n",
    "        test_dataset = FullImageDataset([test_image], [test_label], transform=test_transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        # # Visualize augmented images\n",
    "        # for i in range(2):\n",
    "        #     augmented_image, _ = train_dataset[i]  # Get augmented image\n",
    "        #     plt.imshow(augmented_image.squeeze().numpy(), cmap='gray')  # Visualize in grayscale\n",
    "        #     plt.title(\"Augmented Image\")\n",
    "        #     plt.show()\n",
    "\n",
    "        # Initialize model, loss function, and optimizer\n",
    "        model = CNN_Scratch().to(device)\n",
    "        # model = TransferLearningImageClassifier().to(device)\n",
    "        \n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "        # print(\"class_weights\", class_weights)\n",
    "\n",
    "        # criterion = nn.BCELoss()\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device).unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Testing loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs).squeeze().cpu().numpy()\n",
    "                image_pred_prob.append(outputs)\n",
    "                pred_label = 1 if outputs > 0.5 else 0\n",
    "                image_pred_label.append(pred_label)\n",
    "                true_labels.append(test_label)\n",
    "\n",
    "    # # Save the final trained model\n",
    "    # final_model_path = os.path.join(save_dir, \"rr3_cnn_23022025.pth\")\n",
    "    # torch.save({\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict()\n",
    "    # }, final_model_path)\n",
    "    # print(f\"Final model saved to {final_model_path}\")\n",
    "\n",
    "    return true_labels, image_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../rr3_dataset/preprocessed_images/'\n",
    "\n",
    "label_dict = {\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_158_A1.npy': 1, #'Space Flight'\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_158_B1.npy': 1, #'Space Flight',\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_158_C1.npy': 1, #'Space Flight',\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_158_D1.npy': 1, #'Space Flight',\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_159_A1.npy': 0, #'Ground Control',\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_159_B1.npy': 0, #'Ground Control',\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_159_C1.npy': 1, #'Space Flight',\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_159_D1.npy': 1, #'Space Flight',\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_304_A1.npy': 0, #'Ground Control',\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_304_B1.npy': 0, #'Ground Control',\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_304_C1.npy': 0, #'Ground Control',\n",
    "    image_dir+'GLDS-352_SpatialTranscriptomics_NASA-RR3_Sample_304_D1.npy': 0, #'Ground Control',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = list(label_dict.keys()) # Replace with actual .npy file paths\n",
    "labels = list(label_dict.values())  # Replace with actual labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LOOCV for test image 1/12\n",
      "Processing LOOCV for test image 2/12\n",
      "Processing LOOCV for test image 3/12\n",
      "Processing LOOCV for test image 4/12\n",
      "Processing LOOCV for test image 5/12\n",
      "Processing LOOCV for test image 6/12\n",
      "Processing LOOCV for test image 7/12\n",
      "Processing LOOCV for test image 8/12\n",
      "Processing LOOCV for test image 9/12\n",
      "Processing LOOCV for test image 10/12\n",
      "Processing LOOCV for test image 11/12\n",
      "Processing LOOCV for test image 12/12\n"
     ]
    }
   ],
   "source": [
    "true_labels, image_pred_prob = loocv_full_image_with_augmentation(image_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       " [array(9.913676, dtype=float32),\n",
       "  array(8.717853, dtype=float32),\n",
       "  array(7.8208942, dtype=float32),\n",
       "  array(7.0151954, dtype=float32),\n",
       "  array(-4.285076, dtype=float32),\n",
       "  array(1.2626834, dtype=float32),\n",
       "  array(6.670873, dtype=float32),\n",
       "  array(4.144833, dtype=float32),\n",
       "  array(-10.979938, dtype=float32),\n",
       "  array(-13.445078, dtype=float32),\n",
       "  array(-7.666894, dtype=float32),\n",
       "  array(-7.5089974, dtype=float32)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels, image_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the cutoff value\n",
    "cutoff = 0.5\n",
    "# Convert to 0 and 1 based on the cutoff\n",
    "image_pred_label = (np.array(image_pred_prob) > cutoff).astype(int)\n",
    "image_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV Image-Level Accuracy: 0.92\n",
      "Precision: 0.86\n",
      "Recall: 1.00\n",
      "F1-Score: 0.92\n",
      "AUC Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, image_pred_label)\n",
    "precision = precision_score(true_labels, image_pred_label)\n",
    "recall = recall_score(true_labels, image_pred_label)\n",
    "f1 = f1_score(true_labels, image_pred_label)\n",
    "auc = roc_auc_score(true_labels, image_pred_prob)\n",
    "\n",
    "print(f\"LOOCV Image-Level Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(f\"AUC Score: {auc:.2f}\")\n",
    "\n",
    "# LOOCV Image-Level Accuracy: 0.92\n",
    "# Precision: 0.86\n",
    "# Recall: 1.00\n",
    "# F1-Score: 0.92\n",
    "# AUC Score: 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_TORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
